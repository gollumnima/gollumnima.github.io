---
title: "wecode 6주차_2일 TIL_도전! 크롤링 해보기"
date: "2019-09-03T16:16:04.169Z"
template: "post"
draft: false
slug: "/posts/wecode6_2TIL_crawling"
category: "crawling"
tags:
  - "wecode"
  - "codingbootcamp"
  - "crawling"
  - "backend"
  - "위코드"
description: "위코드 31일차. 오랜만에 약간 백 이야기! 언제 한번 해보고 싶었던 크롤링... 위코드에서 세션이 열려서 드디어 나도 한번 해보았다!"
---

![image.png](https://images.velog.io/post-images/dooreplay/602d4930-d209-11e9-93d3-efc48314c767/image.png)

## 웹 크롤링이란?

Web scraping. 웹사이트에서 원하는 정보를 추출하는 일!
데이터를 긁어온다는 의미에서 아이스크림을 긁는게 생각나서 오늘의 썸네일은 아이스크림!

데이터 분석에 대한 수요가 증가하고 이에 따라 자료를 얻는 원천으로 웹을 자주 드는데, 이 웹을 가져와서 분석을 할 수 있는 자료 형태로 바꾸는게 바로 크롤링!<br/>
http request 통해서 http가 출력해주는 api를 통해 json 데이터를 가져오는 것을 말한다.

## 크롤링 관련 사이트

- 서울시 공공 데이터 포털
- 공공 데이터 포털
- 일별 박스 오피스 API

## 크롤링은 책임감있게..!

크롤링은 자유지만, 웹사이트 무단 크롤링은 _불법!_ <br />
개인 영리적 이익을 취하지 않는 경우나 시스템에 가하지 않으면 크게 문제 되지 않지만 저작권에 대해선 항상 염두해두어야 한다. <br />
<br />Robots에 disabled 써있으면 크롤링 하지말라는 뜻! 잘 확인하고 항상 조심하자!

## how to?

- scrapy : 규모가 클 때 쓰기, python에 있는 크롤링을 위한 라이브러리 <br />
- urllib : fetch하는 느낌으로 쓸 수 있음
- selenium : 기억이 안남... 이름 예쁨 원석 이름 같음ㅋㅋ
- beautiful soup : 파싱된 response를 객체화 시켜주는 툴
- requests - 소셜로그인 기능 구현할 때 백에서 쓰는 라이브러리

<br /> <br />
js에서는...

- puppeteer
- apify
- cheerio
  같은게 있다고 한다.. 내가 세션때 제대로 받아 적은게 맞는지 모르겠지만 여튼ㅋㅋㅋ

## 해보자 실습!

1. requests 깔기
   `pip install requests`
   가상환경 구축은 생략~

2. bs4 깔기
   `pip install bs4`
   (beautiful spoon 4탄임.)

일단 이 두개 깔면 크롤링할 준비 완료쓰~

3. python 파일 하나 만들기

request 하면 화면 전체를 다 가져오는 것 -> 터미널에서 확인할 수 있음

html까지 하면 string만 나와서 복잡쓰
예쁜스푼을 쓰자! 객체형태로 보여줌

```
import requests
from bs4 import BeautifulSoup

req = requests.get('http://www.mnet.com/chart/TOP100/20190826')


html = req.text

soup = BeautifulSoup(html, 'html.parser')

mnet_songs = soup.select(
    'tr > td.MMLItemTitle > div > div.MMLITitle_Box.info > div.MMLITitleSong_Box > a.MMLI_SongInfo'
)

for song in mnet_songs :
    print(song.text)
```

for문을 저렇게 쓰는것도 넘 신기하고....
빽의 세계는 암튼 다 신기함!
<br />그치만 일단 나는 프론트 할거지롱

`python crawling.py` 입력하면 터미널에 결과 나옴

![image.png](https://images.velog.io/post-images/dooreplay/2f3ad700-c88d-11e9-bc39-4fa6ed49fec8/image.png)
<br />핵신기함ㅋㅋㅋㅋㅋㅋㅋㅋㅋ 대박...
근데 프론트랑 다르게 결과물을 웹페이지가 아닌 터미널로 확인해야 한다는 점이 넘 불편쓰..☆

중간에 내쪽에서 뭐가 안 되가지고 중간 설명은 생략...!
결과물 공개해본다

```
import requests
from bs4 import BeautifulSoup
from sqlalchemy import *
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship, sessionmaker
from sqlalchemy.sql import *

engine = create_engine('sqlite:///music.db')
Base = declarative_base()

class Music(Base):
    __tablename__ = 'musics'
    id = Column(Integer, primary_key=True)
    rank = Column(String(50))
    songs = Column(String(50))
    singer = Column(String(50))
    album = Column(String(50))

Music.__table__.create(bind=engine, checkfirst=True)


Session = sessionmaker(bind=engine)
session = Session()

req = requests.get('http://mnet.com/chart/TOP100/20190826')

html = req.text

soup = BeautifulSoup(html, 'html.parser')

rank = soup.select(
    'tr > td.MMLItemRank > div > span'
)

my_songs = soup.select(
    'tr > td.MMLItemTitle > div > div.MMLITitle_Box.info > div.MMLITitleSong_Box > a.MMLI_Song'
)

singer = soup.select(
    'tr > td.MMLItemTitle > div > div.MMLITitle_Box.info > div.MMLITitle_Info > a.MMLIInfo_Artist'
)

album = soup.select(
    'tr > td.MMLItemTitle > div > div.MMLITitle_Box.info > div.MMLITitle_Info > a.MMLIInfo_Album'
)

music_chart = []

for item in zip(rank, my_songs, singer, album):
    music_chart.append(
        {
            'rank' : item[0].text,
            'song' : item[1].text,
            'singer' : item[2].text,
            'album'  : item[3].text,
       }
    )

for element in music_chart:
    print(element)

for element in music_chart:
    result =  Music(rank=element['rank'],
                    songs=element['song'],
                    singer=element['singer'],
                    album=element['album']
    )
    session.add(result)
    session.commit()

request = session.query(Music).all()

for row in request:
   print(row.rank,row.songs,row.singer,row.album)

```

기억이 잘 안나지만 중간에 `pip install sqlalchemy` 설치했음
여튼 중간에 컴터가 멈춰서 강제종료 했던 기억이 있다!

나의 결과물...

![image.png](https://images.velog.io/post-images/dooreplay/c56ff520-c892-11e9-ad06-c7058f1fcfa5/image.png)

아까 노래제목까진 잘 나왔었는데....아놔 웬 이상한 문자열들이 뙇..!
멘토님의 코드를 똑같이 복붙했는데 나만 왜 이런가 했더니만 우분투 문제였다 <br />
<br />애증 말고 증뿐인 우분투.. 증증증이다 증말 ㅡ,.ㅡ
우분투 바보멍청이!

여튼 나도 해봤다, 크롤링!
